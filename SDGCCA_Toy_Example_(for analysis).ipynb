{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e76a862",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4cc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from utils import *\n",
    "from SDGCCA import SDGCCA_3_M\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import shap\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901526b5",
   "metadata": {},
   "source": [
    "### Seed fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1bce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed Setting\n",
    "random_seed = 100\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ad12c",
   "metadata": {},
   "source": [
    "### Train SDGCCA\n",
    "\n",
    "**Toy Dataset**  \n",
    "- Label: Binary\n",
    "- Modality1: n(376) x d1 (18164)\n",
    "- Modality1: n(376) x d2 (19353)\n",
    "- Modality1: n(376) x d3 (309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223b6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SDGCCA(hyper_dict):\n",
    "    # Return List\n",
    "    ensemble_list = {'ACC': [], 'F1': [], 'AUC': [], 'MCC': []}\n",
    "    metric_list = ['ACC', 'F1', 'AUC', 'MCC']\n",
    "    hyper_param_list = []\n",
    "    best_hyper_param_list = []\n",
    "    \n",
    "    # Prepare Toy Dataset\n",
    "    dataset = Toy_Dataset(hyper_dict['random_seed'])\n",
    "\n",
    "    # 5 CV\n",
    "    cv = 0\n",
    "    # Prepare Dataset\n",
    "    [x_train_1, x_val_1, x_test_1], [x_train_2, x_val_2, x_test_2], [x_train_3, x_val_3, x_test_3], \\\n",
    "    [y_train, y_val, y_test] = dataset(cv, tensor=True, device=hyper_dict['device'])\n",
    "\n",
    "    # Define Deep neural network dimension of the each modality\n",
    "    m1_embedding_list = [x_train_1.shape[1]] + hyper_dict['embedding_size']\n",
    "    m2_embedding_list = [x_train_2.shape[1]] + hyper_dict['embedding_size']\n",
    "    m3_embedding_list = [x_train_3.shape[1]] + hyper_dict['embedding_size'][1:]\n",
    "\n",
    "    # Train Label -> One_Hot_Encoding\n",
    "    y_train_onehot = torch.zeros(y_train.shape[0], 2).float().to(hyper_dict['device'])\n",
    "    y_train_onehot[range(y_train.shape[0]), y_train.squeeze()] = 1\n",
    "\n",
    "    # Find Best K by Validation MCC\n",
    "    val_mcc_result_list = []\n",
    "    test_ensemble_dict = {'ACC': [], 'F1': [], 'AUC': [], 'MCC': []}\n",
    "\n",
    "    # Grid search for find best hyperparameter by Validation MCC\n",
    "    for top_k in tqdm(range(1, hyper_dict['max_top_k']+1), desc='Grid seach for find best hyperparameter...'):\n",
    "        for lr in hyper_dict['lr']:\n",
    "            for reg in hyper_dict['reg']:\n",
    "                hyper_param_list.append([top_k, lr, reg])\n",
    "                early_stopping = EarlyStopping(patience=hyper_dict['patience'], delta=hyper_dict['delta'])\n",
    "                best_loss = np.Inf\n",
    "\n",
    "                # Define SDGCCA with 3 modality\n",
    "                model = SDGCCA_3_M(m1_embedding_list, m2_embedding_list, m3_embedding_list, top_k).to(hyper_dict['device'])\n",
    "\n",
    "                # Optimizer\n",
    "                clf_optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "\n",
    "                # Cross Entropy Loss\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                # Model Train\n",
    "                for i in range(hyper_dict['epoch']):\n",
    "                    model.train()\n",
    "\n",
    "                    # Calculate correlation loss\n",
    "                    out1, out2, out3 = model(x_train_1, x_train_2, x_train_3)\n",
    "                    cor_loss = model.cal_loss([out1, out2, out3, y_train_onehot])\n",
    "\n",
    "                    # Calculate classification loss\n",
    "                    clf_optimizer.zero_grad()\n",
    "\n",
    "                    y_hat1, y_hat2, y_hat3, _ = model.predict(x_train_1, x_train_2, x_train_3)\n",
    "                    clf_loss1 = criterion(y_hat1, y_train.squeeze())\n",
    "                    clf_loss2 = criterion(y_hat2, y_train.squeeze())\n",
    "                    clf_loss3 = criterion(y_hat3, y_train.squeeze())\n",
    "\n",
    "                    clf_loss = clf_loss1 + clf_loss2 + clf_loss3\n",
    "\n",
    "                    clf_loss.backward()\n",
    "                    clf_optimizer.step()\n",
    "\n",
    "                    # Model Validation\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        _, _, _, y_ensemble = model.predict(x_val_1, x_val_2, x_val_3)\n",
    "                        val_loss = criterion(y_ensemble, y_val.squeeze())\n",
    "\n",
    "                        early_stopping(val_loss)\n",
    "                        if val_loss < best_loss:\n",
    "                            best_loss = val_loss\n",
    "\n",
    "                        if early_stopping.early_stop:\n",
    "                            break\n",
    "\n",
    "                # Load Best Model\n",
    "                model.eval()\n",
    "\n",
    "                # Model Validation\n",
    "                _, _, _, ensembel_y_hat = model.predict(x_val_1, x_val_2, x_val_3)\n",
    "                y_pred_ensemble = torch.argmax(ensembel_y_hat, 1).cpu().detach().numpy()\n",
    "                y_pred_proba_ensemble = ensembel_y_hat[:, 1].cpu().detach().numpy()            \n",
    "                _, _, _, val_mcc = calculate_metric(y_val.cpu().detach().numpy(), y_pred_ensemble, y_pred_proba_ensemble)\n",
    "                val_mcc_result_list.append(val_mcc)\n",
    "\n",
    "                # Model Tset\n",
    "                _, _, _, ensembel_y_hat = model.predict(x_test_1, x_test_2, x_test_3)\n",
    "                y_pred_ensemble = torch.argmax(ensembel_y_hat, 1).cpu().detach().numpy()\n",
    "                y_pred_proba_ensemble = ensembel_y_hat[:, 1].cpu().detach().numpy()\n",
    "                test_acc, test_f1, test_auc, test_mcc = calculate_metric(y_test.cpu().detach().numpy(), y_pred_ensemble, y_pred_proba_ensemble)\n",
    "                ensemble_result = [test_acc, test_f1, test_auc, test_mcc]\n",
    "                for k, metric in enumerate(metric_list):\n",
    "                    test_ensemble_dict[metric].append(ensemble_result[k])\n",
    "\n",
    "        # Find best K\n",
    "        best_k = np.argmax(val_mcc_result_list)\n",
    "        \n",
    "        # Find best hyperparameter\n",
    "        best_hyper_param_list.append(hyper_param_list[best_k])\n",
    "        \n",
    "        # Append Best K Test Result\n",
    "        for metric in metric_list:\n",
    "            ensemble_list[metric].append(test_ensemble_dict[metric][best_k])\n",
    "\n",
    "    return ensemble_list, best_hyper_param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89e11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLF_Predict(nn.Module):\n",
    "    def __init__(self, embedding_list, top_k, mi_flip=False):\n",
    "        super().__init__()\n",
    "        if mi_flip:\n",
    "            # Embedding List\n",
    "            du0, du1, du2 = embedding_list\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(du0, du1), nn.Tanh(),\n",
    "                nn.Linear(du1, du2), nn.Tanh())\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(du2, top_k, bias=False),\n",
    "                nn.Linear(top_k, 2, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            # Embedding List\n",
    "            du0, du1, du2, du3 = embedding_list\n",
    "\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(du0, du1), nn.Tanh(),\n",
    "                nn.Linear(du1, du2), nn.Tanh(),\n",
    "                nn.Linear(du2, du3), nn.Tanh())\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(du3, top_k, bias=False),\n",
    "                nn.Linear(top_k, 2, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022d894",
   "metadata": {},
   "source": [
    "### Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee055f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dict = {'epoch': 1000, 'delta': 0, 'random_seed': random_seed,\n",
    "              'device': torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "              'lr': [0.0001,0.00001], 'reg': [0, 0.01,0.0001], \n",
    "              'patience': 30, 'embedding_size': [256, 64, 16], 'max_top_k': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d2586",
   "metadata": {},
   "source": [
    "### Model Training & Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5bcfbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af17fb4b93848519e0f1716476c78bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grid seach for find best hyperparameter...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance\n",
      "ACC: 49.34+-3.01 F1: 48.85+-7.71 AUC: 45.98+-5.36 MCC: -0.91+-6.51\n",
      "\n",
      "Best Hyperparameter\n",
      "CV: 1 Best k: 1 Learning Rage: 0.0001 Regularization Term: 0\n",
      "CV: 2 Best k: 2 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 3 Best k: 2 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 4 Best k: 2 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 5 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 6 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 7 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 8 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 9 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
      "CV: 10 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n"
     ]
    }
   ],
   "source": [
    "ensemble_list, hyper = train_SDGCCA(hyper_dict)\n",
    "\n",
    "# Check Performance\n",
    "performance_result = check_mean_std_performance(ensemble_list)\n",
    "\n",
    "print('Test Performance')\n",
    "print('ACC: {} F1: {} AUC: {} MCC: {}'.format(performance_result[0], performance_result[1], performance_result[2], performance_result[3]))\n",
    "\n",
    "print('\\nBest Hyperparameter')\n",
    "for i, h in enumerate(hyper):\n",
    "    print('CV: {} Best k: {} Learning Rage: {} Regularization Term: {}'.format(i+1, h[0], h[1], h[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ddcd4",
   "metadata": {},
   "source": [
    "## Important features (example on CV1)    \n",
    "CV: 1 Best k: 1 Learning Rage: 0.0001 Regularization Term: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4076c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Toy Dataset\n",
    "dataset = Toy_Dataset(hyper_dict['random_seed'])\n",
    "# Return List\n",
    "important_result_pd = pd.DataFrame(\n",
    "    columns=['CV', 'Correlation',\n",
    "             'm1_clf_feature', 'm2_clf_feature', 'm3_clf_feature'])\n",
    "# CV: 1 Best k: 5 Learning Rage: 1e-05 Regularization Term: 0.0001\n",
    "cv = 0\n",
    "top_k = 5\n",
    "lr = 0.0001\n",
    "reg = 0\n",
    "# Prepare Dataset\n",
    "[x_train_1, x_val_1, x_test_1], [x_train_2, x_val_2, x_test_2], [x_train_3, x_val_3, x_test_3], \\\n",
    "[y_train, y_val, y_test] = dataset(cv, tensor=True, device=hyper_dict['device'])\n",
    "\n",
    "# Define Deep neural network dimension of the each modality\n",
    "m1_embedding_list = [x_train_1.shape[1]] + hyper_dict['embedding_size']\n",
    "m2_embedding_list = [x_train_2.shape[1]] + hyper_dict['embedding_size']\n",
    "m3_embedding_list = [x_train_3.shape[1]] + hyper_dict['embedding_size'][1:]\n",
    "\n",
    "# Feature Name\n",
    "m1_name_list = []\n",
    "m2_name_list = []\n",
    "m3_name_list = []\n",
    "for i in range(x_train_1.shape[1]):\n",
    "    m1_name_list.append('Data1_'+str(i))\n",
    "\n",
    "for i in range(x_train_2.shape[1]):\n",
    "    m2_name_list.append('Data2_'+str(i))\n",
    "\n",
    "for i in range(x_train_3.shape[1]):\n",
    "    m3_name_list.append('Data3_'+str(i))\n",
    "# Train Label -> One_Hot_Encoding\n",
    "y_train_onehot = torch.zeros(y_train.shape[0], 2).float().to(hyper_dict['device'])\n",
    "y_train_onehot[range(y_train.shape[0]), y_train.squeeze()] = 1\n",
    "\n",
    "# Define SDGCCA with 3 modality\n",
    "model = SDGCCA_3_M(m1_embedding_list, m2_embedding_list, m3_embedding_list, top_k).to(hyper_dict['device'])\n",
    "early_stopping = EarlyStopping(patience=30, delta=0)\n",
    "best_loss = np.Inf\n",
    "# Optimizer\n",
    "clf_optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Model Train\n",
    "for i in range(hyper_dict['epoch']):\n",
    "    model.train()\n",
    "\n",
    "    # Calculate correlation loss\n",
    "    out1, out2, out3 = model(x_train_1, x_train_2, x_train_3)\n",
    "    cor_loss = model.cal_loss([out1, out2, out3, y_train_onehot])\n",
    "\n",
    "    # Calculate classification loss\n",
    "    clf_optimizer.zero_grad()\n",
    "\n",
    "    y_hat1, y_hat2, y_hat3, _ = model.predict(x_train_1, x_train_2, x_train_3)\n",
    "    clf_loss1 = criterion(y_hat1, y_train.squeeze())\n",
    "    clf_loss2 = criterion(y_hat2, y_train.squeeze())\n",
    "    clf_loss3 = criterion(y_hat3, y_train.squeeze())\n",
    "\n",
    "    clf_loss = clf_loss1 + clf_loss2 + clf_loss3\n",
    "\n",
    "    clf_loss.backward()\n",
    "    clf_optimizer.step()\n",
    "\n",
    "    # Model Validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        _, _, _, y_ensemble = model.predict(x_val_1, x_val_2, x_val_3)\n",
    "        val_loss = criterion(y_ensemble, y_val.squeeze())\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "                # Load Best Model\n",
    "model.eval()\n",
    "\n",
    "# Load U\n",
    "\n",
    "u =  [model.U[0].cpu().detach(), model.U[1].cpu().detach(),\n",
    "                          model.U[2], torch.pinverse(model.U[3]).cpu().detach()]\n",
    "for i in range(len(u)):\n",
    "    u[i] = u[i].to(hyper_dict['device'])\n",
    "\n",
    "\"\"\"\n",
    "x1 = torch.cat((x_train_1, x_val_1, x_test_1), dim=0)\n",
    "x2 = torch.cat((x_train_2, x_val_2, x_test_2), dim=0)\n",
    "x3 = torch.cat((x_train_3, x_val_3, x_test_3), dim=0)\n",
    "\"\"\"\n",
    "x1 = x_train_1\n",
    "x2 = x_train_2\n",
    "x3 = x_train_3\n",
    "\n",
    "output1, output2, output3 = model(x1, x2, x3)\n",
    "\n",
    "# Raw -> Embedding -> PLS Embedding\n",
    "pls_embedding1 = torch.mm(output1, u[0].to(hyper_dict['device']))\n",
    "pls_embedding2 = torch.mm(output2, u[1].to(hyper_dict['device']))\n",
    "pls_embedding3 = torch.mm(output3, u[2].to(hyper_dict['device']))\n",
    "\n",
    "# Correlation - PLS Embedding Dimension=0 -> SVD max eigenvalue\n",
    "correlation = \\\n",
    "    (np.corrcoef(pls_embedding1[:, 0].detach().cpu().numpy(), pls_embedding2[:, 0].detach().cpu().numpy())[0, 1] +\n",
    "     np.corrcoef(pls_embedding1[:, 0].detach().cpu().numpy(), pls_embedding3[:, 0].detach().cpu().numpy())[0, 1] +\n",
    "     np.corrcoef(pls_embedding2[:, 0].detach().cpu().numpy(), pls_embedding3[:, 0].detach().cpu().numpy())[0, 1])/3\n",
    "\n",
    "# Define Modality_Classifier\n",
    "m1_classifier = CLF_Predict(m1_embedding_list,top_k)\n",
    "m2_classifier = CLF_Predict(m2_embedding_list, top_k)\n",
    "m3_classifier = CLF_Predict(m3_embedding_list, top_k, mi_flip=True)\n",
    "\n",
    "# Embedding & Classifier Weight\n",
    "m1_classifier.encoder = model.model1\n",
    "for i, p in enumerate(m1_classifier.classifier.parameters()):\n",
    "    if i == 0:\n",
    "        p.data = u[0].T\n",
    "    else:\n",
    "        p.data = u[3].T\n",
    "m1_classifier = m1_classifier.to(hyper_dict['device'])\n",
    "m1_classifier.eval()\n",
    "\n",
    "# ME Embedding & Classifier Weight\n",
    "m2_classifier.encoder = model.model2\n",
    "for i, p in enumerate(m2_classifier.classifier.parameters()):\n",
    "    if i == 0:\n",
    "        p.data = u[1].T\n",
    "    else:\n",
    "        p.data = u[3].T\n",
    "m2_classifier = m2_classifier.to(hyper_dict['device'])\n",
    "m2_classifier.eval()\n",
    "\n",
    "# ME Embedding & Classifier Weight\n",
    "m3_classifier.encoder = model.model3\n",
    "for i, p in enumerate(m3_classifier.classifier.parameters()):\n",
    "    if i == 0:\n",
    "        p.data = u[2].T\n",
    "    else:\n",
    "        p.data = u[3].T\n",
    "m3_classifier = m3_classifier.to(hyper_dict['device'])\n",
    "m3_classifier.eval()\n",
    "\n",
    "# Modality1 Classification Important Feature\n",
    "explainer_shap = shap.DeepExplainer(m1_classifier, x1)\n",
    "shap_values = explainer_shap.shap_values(x1)\n",
    "clf_m1_important_feature = np.argsort(abs(shap_values[0].mean(0)) + abs(shap_values[1].mean(0)))[::-1][:300]\n",
    "m1_clf_feature = np.array(m1_name_list)[clf_m1_important_feature]\n",
    "\n",
    "# Modality2 Classification Important Feature\n",
    "explainer_shap = shap.DeepExplainer(m2_classifier, x2)\n",
    "shap_values = explainer_shap.shap_values(x2)\n",
    "clf_m2_important_feature = np.argsort(abs(shap_values[0].mean(0)) + abs(shap_values[1].mean(0)))[::-1][:300]\n",
    "m2_clf_feature = np.array(m2_name_list)[clf_m2_important_feature]\n",
    "\n",
    "# Modality3 Classification Important Feature\n",
    "explainer_shap = shap.DeepExplainer(m3_classifier, x3)\n",
    "shap_values = explainer_shap.shap_values(x3)\n",
    "clf_m3_important_feature = np.argsort(abs(shap_values[0].mean(0)) + abs(shap_values[1].mean(0)))[::-1][:30]\n",
    "m3_clf_feature = np.array(m3_name_list)[clf_m3_important_feature]\n",
    "\n",
    "important_dict = {'CV': cv + 1, 'Correlation': correlation,\n",
    "                  'm1_clf_feature': m1_clf_feature, 'm2_clf_feature': m2_clf_feature, 'm3_clf_feature': m3_clf_feature}\n",
    "important_result_pd = important_result_pd.append(important_dict, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484f6e6",
   "metadata": {},
   "source": [
    "Important features (data 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8471b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data1_15348', 'Data1_3288', 'Data1_7453', 'Data1_14143',\n",
       "       'Data1_8334', 'Data1_15793', 'Data1_2745', 'Data1_3290',\n",
       "       'Data1_10881', 'Data1_8440', 'Data1_17406', 'Data1_13185',\n",
       "       'Data1_16885', 'Data1_10418', 'Data1_11008', 'Data1_15480',\n",
       "       'Data1_3389', 'Data1_15811', 'Data1_2911', 'Data1_8826',\n",
       "       'Data1_5446', 'Data1_6685', 'Data1_7821', 'Data1_11394',\n",
       "       'Data1_3617', 'Data1_3240', 'Data1_10628', 'Data1_3962',\n",
       "       'Data1_11800', 'Data1_2014', 'Data1_6199', 'Data1_3267',\n",
       "       'Data1_11176', 'Data1_11232', 'Data1_7718', 'Data1_7414',\n",
       "       'Data1_7506', 'Data1_8976', 'Data1_2843', 'Data1_9036',\n",
       "       'Data1_6295', 'Data1_12579', 'Data1_1244', 'Data1_12091',\n",
       "       'Data1_11251', 'Data1_9918', 'Data1_17066', 'Data1_11186',\n",
       "       'Data1_17288', 'Data1_524', 'Data1_11520', 'Data1_10832',\n",
       "       'Data1_10986', 'Data1_9668', 'Data1_5476', 'Data1_5586',\n",
       "       'Data1_16309', 'Data1_2594', 'Data1_12084', 'Data1_10708',\n",
       "       'Data1_16573', 'Data1_5254', 'Data1_14414', 'Data1_6704',\n",
       "       'Data1_15492', 'Data1_11788', 'Data1_9122', 'Data1_6027',\n",
       "       'Data1_8062', 'Data1_6298', 'Data1_14787', 'Data1_7183',\n",
       "       'Data1_561', 'Data1_2963', 'Data1_1922', 'Data1_16485',\n",
       "       'Data1_2827', 'Data1_9058', 'Data1_7835', 'Data1_14756',\n",
       "       'Data1_1271', 'Data1_6202', 'Data1_6106', 'Data1_13434',\n",
       "       'Data1_14917', 'Data1_4396', 'Data1_2017', 'Data1_1197',\n",
       "       'Data1_2516', 'Data1_6630', 'Data1_9117', 'Data1_15021',\n",
       "       'Data1_12549', 'Data1_8909', 'Data1_4127', 'Data1_9161',\n",
       "       'Data1_576', 'Data1_15286', 'Data1_2831', 'Data1_5705', 'Data1_88',\n",
       "       'Data1_13945', 'Data1_10755', 'Data1_4212', 'Data1_9820',\n",
       "       'Data1_2733', 'Data1_14006', 'Data1_17113', 'Data1_4508',\n",
       "       'Data1_8719', 'Data1_3152', 'Data1_2518', 'Data1_2885',\n",
       "       'Data1_10261', 'Data1_13357', 'Data1_13014', 'Data1_12405',\n",
       "       'Data1_15554', 'Data1_1333', 'Data1_830', 'Data1_15300',\n",
       "       'Data1_15260', 'Data1_5437', 'Data1_15548', 'Data1_8977',\n",
       "       'Data1_10265', 'Data1_5673', 'Data1_3111', 'Data1_2241',\n",
       "       'Data1_875', 'Data1_16915', 'Data1_992', 'Data1_11784',\n",
       "       'Data1_2620', 'Data1_18025', 'Data1_14856', 'Data1_12585',\n",
       "       'Data1_15791', 'Data1_8313', 'Data1_10334', 'Data1_10432',\n",
       "       'Data1_10460', 'Data1_17094', 'Data1_3562', 'Data1_8704',\n",
       "       'Data1_11270', 'Data1_10228', 'Data1_5958', 'Data1_8462',\n",
       "       'Data1_813', 'Data1_10917', 'Data1_9771', 'Data1_14382',\n",
       "       'Data1_11092', 'Data1_1422', 'Data1_10435', 'Data1_11381',\n",
       "       'Data1_3306', 'Data1_17246', 'Data1_14362', 'Data1_1937',\n",
       "       'Data1_12461', 'Data1_7240', 'Data1_7030', 'Data1_17595',\n",
       "       'Data1_14802', 'Data1_15378', 'Data1_14738', 'Data1_6617',\n",
       "       'Data1_10323', 'Data1_4284', 'Data1_8618', 'Data1_8223',\n",
       "       'Data1_11990', 'Data1_14692', 'Data1_848', 'Data1_13177',\n",
       "       'Data1_5912', 'Data1_14887', 'Data1_9516', 'Data1_11551',\n",
       "       'Data1_61', 'Data1_1542', 'Data1_14491', 'Data1_7638',\n",
       "       'Data1_2293', 'Data1_9315', 'Data1_4791', 'Data1_10306',\n",
       "       'Data1_11094', 'Data1_13687', 'Data1_1310', 'Data1_2905',\n",
       "       'Data1_392', 'Data1_4612', 'Data1_6217', 'Data1_3859',\n",
       "       'Data1_7746', 'Data1_7623', 'Data1_7433', 'Data1_4479',\n",
       "       'Data1_11490', 'Data1_4425', 'Data1_7370', 'Data1_3535',\n",
       "       'Data1_2644', 'Data1_181', 'Data1_7200', 'Data1_6570',\n",
       "       'Data1_14675', 'Data1_5897', 'Data1_17169', 'Data1_8219',\n",
       "       'Data1_15189', 'Data1_14298', 'Data1_1228', 'Data1_5460',\n",
       "       'Data1_15988', 'Data1_10248', 'Data1_2435', 'Data1_7931',\n",
       "       'Data1_1861', 'Data1_2037', 'Data1_16565', 'Data1_6896',\n",
       "       'Data1_9558', 'Data1_14236', 'Data1_14403', 'Data1_3951',\n",
       "       'Data1_1983', 'Data1_9240', 'Data1_10235', 'Data1_3323',\n",
       "       'Data1_10793', 'Data1_11553', 'Data1_11046', 'Data1_13617',\n",
       "       'Data1_11254', 'Data1_11802', 'Data1_12188', 'Data1_11835',\n",
       "       'Data1_16702', 'Data1_7420', 'Data1_11382', 'Data1_16994',\n",
       "       'Data1_16326', 'Data1_12230', 'Data1_11590', 'Data1_2211',\n",
       "       'Data1_15718', 'Data1_11903', 'Data1_4270', 'Data1_8945',\n",
       "       'Data1_17484', 'Data1_2970', 'Data1_17131', 'Data1_302',\n",
       "       'Data1_987', 'Data1_13820', 'Data1_5329', 'Data1_8717',\n",
       "       'Data1_13708', 'Data1_7280', 'Data1_15700', 'Data1_7363',\n",
       "       'Data1_3725', 'Data1_17819', 'Data1_14199', 'Data1_16870',\n",
       "       'Data1_5211', 'Data1_4356', 'Data1_4840', 'Data1_10722',\n",
       "       'Data1_9983', 'Data1_17441', 'Data1_16367', 'Data1_14222',\n",
       "       'Data1_17442', 'Data1_9484', 'Data1_8847', 'Data1_9842',\n",
       "       'Data1_926', 'Data1_17491', 'Data1_15227', 'Data1_10137',\n",
       "       'Data1_13429', 'Data1_4176', 'Data1_16080', 'Data1_5213',\n",
       "       'Data1_2680', 'Data1_16692', 'Data1_9175', 'Data1_6951',\n",
       "       'Data1_8122', 'Data1_11883', 'Data1_14927', 'Data1_680',\n",
       "       'Data1_17500', 'Data1_7171', 'Data1_4060'], dtype='<U11')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_clf_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
